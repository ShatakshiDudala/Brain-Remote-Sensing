import streamlit as st
import numpy as np
import pandas as pd
from PIL import Image
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import folium
from streamlit_folium import st_folium
import cv2
import io
import base64
from datetime import datetime
import zipfile
import json
import hashlib
import time

# Import custom modules
import sys
import os
sys.path.append(os.path.dirname(__file__))

from models.cnn_model import CNNModel
from models.snn_model import SNNModel
from models.transformer_model import TransformerModel
from utils.preprocessing import preprocess_image, extract_roi
from utils.inference import run_inference, calculate_metrics
from utils.visualization import create_attention_map, create_segmentation_overlay

# Page configuration
st.set_page_config(
    page_title="Brain-Inspired Remote Sensing AI",
    page_icon="ğŸ›°ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2c3e50;
        margin: 1rem 0;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .brain-inspired {
        background: linear-gradient(45deg, #ff6b6b, #4ecdc4);
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: bold;
    }
    .auth-container {
        max-width: 400px;
        margin: 2rem auto;
        padding: 2rem;
        border-radius: 15px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        box-shadow: 0 10px 30px rgba(0,0,0,0.2);
    }
    .auth-header {
        text-align: center;
        font-size: 2rem;
        margin-bottom: 1.5rem;
        font-weight: bold;
    }
    .welcome-message {
        background: linear-gradient(45deg, #4ecdc4, #44a08d);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

def initialize_session_state():
    """Initialize session state variables including authentication"""
    # Authentication states
    if 'authenticated' not in st.session_state:
        st.session_state.authenticated = False
    if 'username' not in st.session_state:
        st.session_state.username = ""
    if 'user_data' not in st.session_state:
        st.session_state.user_data = {}
    if 'users_db' not in st.session_state:
        # Simple in-memory user database (in production, use proper database)
        st.session_state.users_db = {
            'admin': {
                'password': hash_password('admin123'),
                'email': 'admin@brainrs.ai',
                'role': 'administrator',
                'created_at': datetime.now()
            }
        }
    
    # Existing application states
    if 'uploaded_images' not in st.session_state:
        st.session_state.uploaded_images = []
    if 'processed_results' not in st.session_state:
        st.session_state.processed_results = {}
    if 'model_history' not in st.session_state:
        st.session_state.model_history = []
    if 'user_feedback' not in st.session_state:
        st.session_state.user_feedback = []

def hash_password(password):
    """Hash password using SHA-256"""
    return hashlib.sha256(password.encode()).hexdigest()

def verify_password(password, hashed):
    """Verify password against hash"""
    return hash_password(password) == hashed

def show_auth_page():
    """Show authentication page with login and signup"""
    st.markdown('<div class="auth-container">', unsafe_allow_html=True)
    
    # Authentication tabs
    tab1, tab2 = st.tabs(["ğŸ” Login", "ğŸ“ Sign Up"])
    
    with tab1:
        st.markdown('<div class="auth-header">ğŸ” Login</div>', unsafe_allow_html=True)
        
        with st.form("login_form"):
            username = st.text_input("ğŸ‘¤ Username", placeholder="Enter your username")
            password = st.text_input("ğŸ”’ Password", type="password", placeholder="Enter your password")
            remember_me = st.checkbox("Remember me")
            
            login_button = st.form_submit_button("ğŸš€ Login", use_container_width=True)
            
            if login_button:
                if authenticate_user(username, password):
                    st.session_state.authenticated = True
                    st.session_state.username = username
                    st.session_state.user_data = st.session_state.users_db[username]
                    st.success(f"âœ… Welcome back, {username}!")
                    st.rerun()
                else:
                    st.error("âŒ Invalid username or password!")
        
        # Demo credentials info
        with st.expander("ğŸ” Demo Credentials"):
            st.info("""
            **Demo Account:**
            - Username: `admin`
            - Password: `admin123`
            
            Or create your own account using the Sign Up tab!
            """)
    
    with tab2:
        st.markdown('<div class="auth-header">ğŸ“ Sign Up</div>', unsafe_allow_html=True)
        
        with st.form("signup_form"):
            new_username = st.text_input("ğŸ‘¤ Choose Username", placeholder="Enter desired username")
            new_email = st.text_input("ğŸ“§ Email", placeholder="Enter your email")
            new_password = st.text_input("ğŸ”’ Password", type="password", placeholder="Create a password")
            confirm_password = st.text_input("ğŸ”’ Confirm Password", type="password", placeholder="Confirm your password")
            
            # Role selection
            role = st.selectbox("ğŸ‘¥ Role", ["researcher", "student", "professional", "hobbyist"])
            
            # Terms and conditions
            agree_terms = st.checkbox("I agree to the Terms and Conditions")
            
            signup_button = st.form_submit_button("ğŸ¯ Create Account", use_container_width=True)
            
            if signup_button:
                if create_user_account(new_username, new_email, new_password, confirm_password, role, agree_terms):
                    st.success("ğŸ‰ Account created successfully! Please login with your credentials.")
                    time.sleep(2)
                    st.rerun()
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 1rem;'>
        ğŸ§  Brain-Inspired Remote Sensing AI Platform<br>
        Secure â€¢ Intelligent â€¢ Sustainable
    </div>
    """, unsafe_allow_html=True)

def authenticate_user(username, password):
    """Authenticate user credentials"""
    if username in st.session_state.users_db:
        stored_password = st.session_state.users_db[username]['password']
        return verify_password(password, stored_password)
    return False

def create_user_account(username, email, password, confirm_password, role, agree_terms):
    """Create new user account with validation"""
    # Validation
    if not username or not email or not password:
        st.error("âŒ Please fill in all required fields!")
        return False
    
    if username in st.session_state.users_db:
        st.error("âŒ Username already exists! Please choose a different one.")
        return False
    
    if len(password) < 6:
        st.error("âŒ Password must be at least 6 characters long!")
        return False
    
    if password != confirm_password:
        st.error("âŒ Passwords do not match!")
        return False
    
    if not agree_terms:
        st.error("âŒ Please agree to the Terms and Conditions!")
        return False
    
    # Email validation (basic)
    if '@' not in email or '.' not in email:
        st.error("âŒ Please enter a valid email address!")
        return False
    
    # Create account
    st.session_state.users_db[username] = {
        'password': hash_password(password),
        'email': email,
        'role': role,
        'created_at': datetime.now(),
        'last_login': None,
        'total_logins': 0
    }
    
    return True

def show_user_profile():
    """Show user profile information"""
    if not st.session_state.authenticated:
        return
    
    user_data = st.session_state.user_data
    
    st.markdown("### ğŸ‘¤ User Profile")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(f"**Username:** {st.session_state.username}")
        st.markdown(f"**Email:** {user_data.get('email', 'N/A')}")
        st.markdown(f"**Role:** {user_data.get('role', 'user').title()}")
    
    with col2:
        st.markdown(f"**Member Since:** {user_data.get('created_at', 'N/A')}")
        st.markdown(f"**Total Logins:** {user_data.get('total_logins', 0)}")
        st.markdown(f"**Last Login:** {user_data.get('last_login', 'N/A')}")
    
    # Logout button
    if st.button("ğŸšª Logout", type="secondary"):
        logout_user()

def logout_user():
    """Logout user and clear session"""
    st.session_state.authenticated = False
    st.session_state.username = ""
    st.session_state.user_data = {}
    st.success("ğŸ‘‹ You have been logged out successfully!")
    st.rerun()

def main():
    initialize_session_state()
    
    # Check authentication
    if not st.session_state.authenticated:
        show_auth_page()
        return
    
    # Update login stats
    if 'last_login' not in st.session_state.user_data or st.session_state.user_data['last_login'] != datetime.now().date():
        st.session_state.users_db[st.session_state.username]['last_login'] = datetime.now().date()
        st.session_state.users_db[st.session_state.username]['total_logins'] = st.session_state.users_db[st.session_state.username].get('total_logins', 0) + 1
        st.session_state.user_data = st.session_state.users_db[st.session_state.username]
    
    # Welcome message
    st.markdown(f'''
    <div class="welcome-message">
        ğŸ‰ Welcome back, <strong>{st.session_state.username}!</strong> 
        Ready to explore brain-inspired remote sensing? ğŸ§ ğŸ›°ï¸
    </div>
    ''', unsafe_allow_html=True)
    
    # Main header
    st.markdown('<h1 class="main-header">ğŸ§  Brain-Inspired Remote Sensing AI Platform</h1>', unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown('<div class="brain-inspired">Neural Remote Sensing</div>', unsafe_allow_html=True)
        
        # User info in sidebar
        st.markdown("---")
        st.markdown(f"ğŸ‘¤ **{st.session_state.username}**")
        st.markdown(f"ğŸ­ {st.session_state.user_data.get('role', 'user').title()}")
        
        if st.button("ğŸ‘¤ Profile", use_container_width=True):
            st.session_state.show_profile = True
        
        if st.button("ğŸšª Logout", type="secondary", use_container_width=True):
            logout_user()
        
        st.markdown("---")
        
        # Navigation
        page = st.selectbox(
            "Navigation",
            ["ğŸ  Home", "ğŸ“¤ Upload & Process", "ğŸ§  Model Selection", "ğŸ“Š Analysis & Results", "ğŸ“ˆ Comparison & Metrics", "ğŸ—ºï¸ Interactive Map", "âš™ï¸ Advanced Settings", "ğŸ“š Research Mode"]
        )
        
        # Model status
        st.markdown("### ğŸ”„ Model Status")
        st.success("âœ… CNN Ready")
        st.success("âœ… SNN Ready")
        st.info("âš¡ Transformer Ready")
        
        # Quick stats
        st.markdown("### ğŸ“Š Session Stats")
        st.metric("Images Processed", len(st.session_state.uploaded_images))
        st.metric("Models Used", len(set(r.get('model', '') for r in st.session_state.processed_results.values())))
        st.metric("Accuracy (Avg)", f"{np.mean([r.get('accuracy', 0) for r in st.session_state.processed_results.values()]) * 100:.1f}%" if st.session_state.processed_results else "0%")
    
    # Show profile if requested
    if st.session_state.get('show_profile', False):
        show_user_profile()
        if st.button("â† Back to Main"):
            st.session_state.show_profile = False
            st.rerun()
        return
    
    # Main content area
    if page == "ğŸ  Home":
        show_home_page()
    elif page == "ğŸ“¤ Upload & Process":
        show_upload_page()
    elif page == "ğŸ§  Model Selection":
        show_model_selection()
    elif page == "ğŸ“Š Analysis & Results":
        show_analysis_page()
    elif page == "ğŸ“ˆ Comparison & Metrics":
        show_comparison_page()
    elif page == "ğŸ—ºï¸ Interactive Map":
        show_interactive_map()
    elif page == "âš™ï¸ Advanced Settings":
        show_advanced_settings()
    elif page == "ğŸ“š Research Mode":
        show_research_mode()

def show_home_page():
    """Home page with project overview and features"""
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("## ğŸŒ Welcome to Brain-Inspired Remote Sensing")
        st.markdown("""
        This cutting-edge platform combines **neuroscience-inspired AI models** with **satellite remote sensing** 
        to provide intelligent interpretation of Earth observation data.
        
        ### ğŸ§  Brain-Inspired Features:
        - **Spiking Neural Networks (SNNs)** - Energy-efficient, temporal processing
        - **Attention Mechanisms** - Human-like focus on important regions
        - **Capsule Networks** - Hierarchical part-whole relationships
        - **Vision Transformers** - Global context understanding
        
        ### ğŸ›°ï¸ Remote Sensing Applications:
        - Land cover classification
        - Urban development monitoring
        - Crop health assessment
        - Disaster detection (floods, fires, storms)
        - Change detection over time
        - Environmental monitoring
        """)
        
        # Feature highlights
        st.markdown("### âœ¨ Key Capabilities")
        
        features = [
            ("ğŸ¯", "Multi-Model Comparison", "Compare traditional CNNs with brain-inspired architectures"),
            ("ğŸ“Š", "Real-time Analysis", "Process images with instant feedback and metrics"),
            ("ğŸ—ºï¸", "Interactive Mapping", "Visualize results on interactive maps"),
            ("ğŸ“ˆ", "Performance Metrics", "Comprehensive evaluation with confusion matrices, IoU, etc."),
            ("ğŸ’¾", "Export Results", "Download predictions in GIS-compatible formats"),
            ("ğŸ”¬", "Explainable AI", "Understand model decisions with attention maps")
        ]
        
        for icon, title, desc in features:
            st.markdown(f"**{icon} {title}**: {desc}")
    
    with col2:
        st.markdown("### ğŸ“Š Live Demo")
        
        # Sample visualization
        sample_data = np.random.rand(50, 50, 3)
        st.image(sample_data, caption="Sample Remote Sensing Image", use_container_width=True)
        
        # Quick metrics
        metrics_col1, metrics_col2 = st.columns(2)
        with metrics_col1:
            st.metric("Accuracy", "94.2%", "2.1%")
            st.metric("Processing Speed", "0.3s", "-0.1s")
        with metrics_col2:
            st.metric("Model Efficiency", "89%", "5%")
            st.metric("Coverage Area", "25kmÂ²", "10kmÂ²")
        
        # Getting started
        st.markdown("### ğŸš€ Quick Start")
        if st.button("ğŸ“¤ Upload Your First Image", type="primary"):
            st.switch_page("Upload & Process")
        
        st.markdown("### ğŸ“š Learn More")
        st.markdown("""
        - [ğŸ“– Documentation](https://github.com/your-repo)
        - [ğŸ¥ Video Tutorials](https://youtube.com)
        - [ğŸ“§ Contact Support](mailto:support@example.com)
        """)

def show_upload_page():
    """Upload and preprocessing page"""
    st.markdown('<h2 class="section-header">ğŸ“¤ Upload & Preprocess Images</h2>', unsafe_allow_html=True)
    
    # Upload section
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ“ Upload Remote Sensing Data")
        
        # File uploader
        uploaded_files = st.file_uploader(
            "Choose images (GeoTIFF, PNG, JPG)",
            type=['tif', 'tiff', 'png', 'jpg', 'jpeg'],
            accept_multiple_files=True,
            help="Upload satellite/aerial images for analysis"
        )
        
        if uploaded_files:
            st.success(f"âœ… Uploaded {len(uploaded_files)} file(s)")
            
            # Display uploaded files
            for i, file in enumerate(uploaded_files):
                with st.expander(f"ğŸ“„ {file.name}"):
                    # Display image
                    image = Image.open(file)
                    st.image(image, caption=f"Original: {file.name}", use_container_width=True)
                    
                    # Image info
                    st.write(f"**Size**: {image.size}")
                    st.write(f"**Mode**: {image.mode}")
                    st.write(f"**Format**: {image.format}")
    
    with col2:
        st.markdown("### âš™ï¸ Preprocessing Options")
        
        # Preprocessing parameters
        with st.form("preprocessing_form"):
            st.markdown("#### ğŸ›ï¸ Image Processing")
            
            resize_option = st.selectbox("Resize Method", ["Keep Original", "Fixed Size", "Aspect Ratio"])
            if resize_option == "Fixed Size":
                width = st.number_input("Width", 256, 2048, 512)
                height = st.number_input("Height", 256, 2048, 512)
            
            # Band selection
            st.markdown("#### ğŸ“¡ Band Selection")
            bands = st.multiselect(
                "Select Bands",
                ["Red", "Green", "Blue", "NIR", "SWIR1", "SWIR2"],
                default=["Red", "Green", "Blue"]
            )
            
            # Normalization
            st.markdown("#### ğŸ“Š Normalization")
            norm_method = st.selectbox("Method", ["Min-Max", "Z-Score", "Robust", "None"])
            
            # ROI Selection
            st.markdown("#### ğŸ¯ Region of Interest")
            use_roi = st.checkbox("Select ROI")
            if use_roi:
                roi_x = st.slider("X Start", 0, 100, 0)
                roi_y = st.slider("Y Start", 0, 100, 0)
                roi_width = st.slider("Width %", 10, 100, 50)
                roi_height = st.slider("Height %", 10, 100, 50)
            
            # Enhancement
            st.markdown("#### âœ¨ Enhancement")
            apply_denoising = st.checkbox("Apply Denoising")
            enhance_contrast = st.checkbox("Enhance Contrast")
            
            process_button = st.form_submit_button("ğŸ”„ Process Images", type="primary")
    
    # Processing results
    if uploaded_files and process_button:
        st.markdown("---")
        st.markdown("### ğŸ”„ Processing Results")
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        processed_images = []
        
        for i, file in enumerate(uploaded_files):
            status_text.text(f"Processing {file.name}...")
            progress_bar.progress((i + 1) / len(uploaded_files))
            
            # Simulate processing
            image = Image.open(file)
            
            # Apply preprocessing (simplified)
            processed_img = preprocess_image(
                np.array(image), 
                bands=bands,
                normalization=norm_method,
                denoise=apply_denoising,
                enhance_contrast=enhance_contrast
            )
            
            processed_images.append({
                'name': file.name,
                'original': image,
                'processed': processed_img,
                'metadata': {
                    'bands': bands,
                    'normalization': norm_method,
                    'size': image.size
                }
            })
            
            # Store in session state
            st.session_state.uploaded_images.append({
                'name': file.name,
                'data': processed_img,
                'metadata': {
                    'bands': bands,
                    'normalization': norm_method,
                    'processing_time': datetime.now()
                }
            })
        
        status_text.text("âœ… Processing complete!")
        
        # Display processed results
        st.markdown("### ğŸ“¸ Before vs After")
        
        for result in processed_images:
            st.markdown(f"#### {result['name']}")
            col1, col2 = st.columns(2)
            
            with col1:
                st.image(result['original'], caption="Original", use_container_width=True, clamp=True)
            
            with col2:
                st.image(result['processed'], caption="Processed", use_container_width=True, clamp=True)
            
            # Metadata
            with st.expander("ğŸ“‹ Processing Details"):
                st.json(result['metadata'])

def show_model_selection():
    """Model selection and configuration page"""
    st.markdown('<h2 class="section-header">ğŸ§  Brain-Inspired Model Selection</h2>', unsafe_allow_html=True)
    
    if not st.session_state.uploaded_images:
        st.warning("âš ï¸ Please upload images first!")
        return
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### ğŸ¤– Available Models")
        
        # Model selection tabs
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ”¥ CNN", "âš¡ SNN", "ğŸ¯ Transformer", "ğŸ“Š Compare All"])
        
        with tab1:
            st.markdown("#### ğŸ”¥ Convolutional Neural Network")
            st.markdown("""
            **Traditional CNN Architecture**
            - Standard convolutional layers
            - Max pooling and dropout
            - Efficient for image classification
            - Baseline comparison model
            """)
            
            with st.form("cnn_config"):
                st.markdown("##### Configuration")
                cnn_layers = st.slider("Number of Layers", 3, 12, 6)
                cnn_filters = st.selectbox("Base Filters", [32, 64, 128], index=1)
                cnn_dropout = st.slider("Dropout Rate", 0.0, 0.5, 0.3)
                cnn_activation = st.selectbox("Activation", ["relu", "leaky_relu", "swish"])
                
                if st.form_submit_button("ğŸš€ Configure CNN"):
                    st.success("âœ… CNN configured successfully!")
        
        with tab2:
            st.markdown("#### âš¡ Spiking Neural Network")
            st.markdown("""
            **Brain-Inspired SNN**
            - Spike-based information processing
            - Energy-efficient computation
            - Temporal dynamics modeling
            - Biologically plausible learning
            """)
            
            with st.form("snn_config"):
                st.markdown("##### Configuration")
                snn_threshold = st.slider("Spike Threshold", 0.1, 1.0, 0.5)
                snn_decay = st.slider("Membrane Decay", 0.1, 0.9, 0.7)
                snn_timesteps = st.slider("Time Steps", 10, 100, 50)
                snn_neuron_type = st.selectbox("Neuron Model", ["LIF", "Izhikevich", "AdEx"])
                
                if st.form_submit_button("âš¡ Configure SNN"):
                    st.success("âš¡ SNN configured successfully!")
        
        with tab3:
            st.markdown("#### ğŸ¯ Vision Transformer")
            st.markdown("""
            **Attention-Based Architecture**
            - Global context understanding
            - Self-attention mechanisms
            - Patch-based processing
            - State-of-the-art performance
            """)
            
            with st.form("transformer_config"):
                st.markdown("##### Configuration")
                vit_patch_size = st.selectbox("Patch Size", [8, 16, 32], index=1)
                vit_embed_dim = st.selectbox("Embedding Dim", [256, 512, 768], index=1)
                vit_heads = st.slider("Attention Heads", 4, 16, 8)
                vit_layers = st.slider("Transformer Layers", 6, 24, 12)
                
                if st.form_submit_button("ğŸ¯ Configure Transformer"):
                    st.success("ğŸ¯ Transformer configured successfully!")
        
        with tab4:
            st.markdown("#### ğŸ“Š Model Comparison")
            st.markdown("Run multiple models simultaneously for comparison")
            
            models_to_compare = st.multiselect(
                "Select Models",
                ["CNN", "SNN", "Transformer"],
                default=["CNN", "SNN"]
            )
            
            if st.button("ğŸ”„ Run Comparison", type="primary"):
                run_model_comparison(models_to_compare)
    
    with col2:
        st.markdown("### ğŸ¯ Task Selection")
        
        task = st.selectbox(
            "Remote Sensing Task",
            [
                "Land Cover Classification",
                "Urban Detection", 
                "Crop Health Analysis",
                "Flood Detection",
                "Fire Detection",
                "Change Detection",
                "Object Counting"
            ]
        )
        
        st.markdown("### ğŸ“Š Model Performance Preview")
        
        # Performance comparison chart
        model_data = {
            'Model': ['CNN', 'SNN', 'Transformer'],
            'Accuracy': [87.5, 91.2, 94.1],
            'Speed (ms)': [45, 120, 89],
            'Energy': [100, 23, 78]
        }
        
        df = pd.DataFrame(model_data)
        
        fig = make_subplots(
            rows=1, cols=3,
            subplot_titles=('Accuracy %', 'Speed (ms)', 'Energy Efficiency'),
            specs=[[{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}]]
        )
        
        colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']
        
        for i, metric in enumerate(['Accuracy', 'Speed (ms)', 'Energy']):
            fig.add_trace(
                go.Bar(x=df['Model'], y=df[metric], marker_color=colors, name=metric),
                row=1, col=i+1
            )
        
        fig.update_layout(height=300, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
        
        # Model recommendations
        st.markdown("### ğŸ’¡ Recommendations")
        
        if task == "Land Cover Classification":
            st.info("ğŸ¯ **Transformer** recommended for global context")
        elif task == "Real-time Detection":
            st.info("âš¡ **SNN** recommended for energy efficiency")
        else:
            st.info("ğŸ”¥ **CNN** recommended for balanced performance")

def show_analysis_page():
    """Analysis and results visualization page"""
    st.markdown('<h2 class="section-header">ğŸ“Š Analysis & Results</h2>', unsafe_allow_html=True)
    
    if not st.session_state.uploaded_images:
        st.warning("âš ï¸ Please upload and process images first!")
        return
    
    # Image selection
    selected_image = st.selectbox(
        "Select Image for Analysis",
        [img['name'] for img in st.session_state.uploaded_images]
    )
    
    if selected_image:
        # Find selected image data
        img_data = next(img for img in st.session_state.uploaded_images if img['name'] == selected_image)
        
        # Analysis tabs
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ–¼ï¸ Visual Results", "ğŸ¯ Attention Maps", "ğŸ“ˆ Metrics", "ğŸ” Explainable AI"])
        
        with tab1:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("#### Original Image")
                st.image(img_data['data'], use_container_width=True)
            
            with col2:
                st.markdown("#### Prediction Overlay")
                # Create mock segmentation overlay
                overlay = create_segmentation_overlay(img_data['data'])
                st.image(overlay, use_column_width=True)
            
            with col3:
                st.markdown("#### Class Probabilities")
                # Mock class predictions
                classes = ['Water', 'Vegetation', 'Urban', 'Bare Soil', 'Agriculture']
                probabilities = np.random.dirichlet(np.ones(5)) * 100
                
                fig = px.bar(
                    x=classes, y=probabilities,
                    color=probabilities,
                    color_continuous_scale='viridis'
                )
                fig.update_layout(height=300)
                st.plotly_chart(fig, use_container_width=True)
        
        with tab2:
            st.markdown("#### ğŸ§  Brain-Inspired Attention Visualization")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("##### SNN Attention Pattern")
                attention_map = create_attention_map(img_data['data'], 'snn')
                st.image(attention_map, use_container_width=True)
                st.caption("Spike-based attention focuses on temporal changes")
            
            with col2:
                st.markdown("##### Transformer Attention")
                attention_map = create_attention_map(img_data['data'], 'transformer')
                st.image(attention_map, use_container_width=True)
                st.caption("Self-attention captures global relationships")
            
            # Attention analysis
            st.markdown("#### ğŸ“Š Attention Analysis")
            
            attention_data = {
                'Region': ['Forest', 'Urban', 'Water', 'Agriculture', 'Roads'],
                'SNN Attention': np.random.rand(5),
                'Transformer Attention': np.random.rand(5)
            }
            
            df_attention = pd.DataFrame(attention_data)
            
            fig = px.scatter(
                df_attention, x='SNN Attention', y='Transformer Attention',
                text='Region', size_max=20
            )
            fig.update_traces(textposition="top center")
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            st.markdown("#### ğŸ“Š Performance Metrics")
            
            # Metrics summary
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Overall Accuracy", "92.3%", "1.2%")
            with col2:
                st.metric("IoU Score", "0.847", "0.023")
            with col3:
                st.metric("Processing Time", "0.34s", "-0.08s")
            with col4:
                st.metric("Energy Efficiency", "78%", "12%")
            
            # Detailed metrics
            st.markdown("##### Confusion Matrix")
            
            # Mock confusion matrix
            confusion_data = np.random.randint(0, 100, (5, 5))
            np.fill_diagonal(confusion_data, np.random.randint(80, 100, 5))
            
            fig = px.imshow(
                confusion_data,
                labels=dict(x="Predicted", y="Actual", color="Count"),
                x=['Water', 'Vegetation', 'Urban', 'Bare Soil', 'Agriculture'],
                y=['Water', 'Vegetation', 'Urban', 'Bare Soil', 'Agriculture'],
                color_continuous_scale='blues'
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Classification report
            st.markdown("##### Classification Report")
            
            report_data = {
                'Class': ['Water', 'Vegetation', 'Urban', 'Bare Soil', 'Agriculture', 'Macro Avg'],
                'Precision': [0.89, 0.94, 0.87, 0.82, 0.91, 0.89],
                'Recall': [0.92, 0.91, 0.89, 0.85, 0.88, 0.89],
                'F1-Score': [0.90, 0.92, 0.88, 0.83, 0.89, 0.88]
            }
            
            df_report = pd.DataFrame(report_data)
            st.dataframe(df_report, use_container_width=True)
        
        with tab4:
            st.markdown("#### ğŸ” Explainable AI Analysis")
            
            # Grad-CAM visualization
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("##### Grad-CAM Heatmap")
                gradcam = create_attention_map(img_data['data'], 'gradcam')
                st.image(gradcam, use_container_width=True)
            
            with col2:
                st.markdown("##### Saliency Map")
                saliency = create_attention_map(img_data['data'], 'saliency')
                st.image(saliency, use_container_width=True)
            
            # Feature importance
            st.markdown("##### Feature Importance Analysis")
            
            features = ['Spectral Bands', 'Texture', 'Edge Information', 'Spatial Context', 'Temporal Changes']
            importance = np.random.rand(5)
            
            fig = px.bar(
                x=features, y=importance,
                color=importance,
                color_continuous_scale='plasma'
            )
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
            
            # Model decision explanation
            st.markdown("##### Model Decision Explanation")
            st.info("""
            ğŸ§  **Brain-Inspired Analysis**: The SNN model detected water bodies by analyzing temporal 
            spike patterns, while the attention mechanism focused on spectral signatures in the NIR band. 
            The model's confidence is highest in areas with consistent spectral characteristics.
            """)

def show_comparison_page():
    """Model comparison and benchmarking page"""
    st.markdown('<h2 class="section-header">ğŸ“ˆ Model Comparison & Benchmarking</h2>', unsafe_allow_html=True)
    
    # Comparison overview
    st.markdown("### ğŸ”„ Multi-Model Performance Analysis")
    
    # Performance metrics comparison
    model_comparison_data = {
        'Model': ['Traditional CNN', 'Spiking Neural Network', 'Vision Transformer', 'Hybrid CNN-SNN'],
        'Accuracy (%)': [87.2, 91.5, 94.1, 92.8],
        'Processing Time (ms)': [45, 120, 89, 67],
        'Energy Efficiency': [100, 23, 78, 45],
        'Memory Usage (MB)': [156, 89, 234, 123],
        'Interpretability': [6, 9, 7, 8]
    }
    
    df_comparison = pd.DataFrame(model_comparison_data)
    
    # Radar chart for comprehensive comparison
    categories = ['Accuracy', 'Speed', 'Energy Eff.', 'Memory Eff.', 'Interpretability']
    
    fig = go.Figure()
    
    for i, model in enumerate(df_comparison['Model']):
        values = [
            df_comparison.iloc[i]['Accuracy (%)'] / 100 * 10,
            (200 - df_comparison.iloc[i]['Processing Time (ms)']) / 200 * 10,
            df_comparison.iloc[i]['Energy Efficiency'] / 100 * 10,
            (300 - df_comparison.iloc[i]['Memory Usage (MB)']) / 300 * 10,
            df_comparison.iloc[i]['Interpretability']
        ]
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name=model,
            line=dict(width=2)
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 10]
            )),
        showlegend=True,
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Detailed comparison tables
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ“Š Performance Metrics")
        st.dataframe(df_comparison[['Model', 'Accuracy (%)', 'Processing Time (ms)']], use_container_width=True)
        
        # Processing time comparison
        fig = px.bar(
            df_comparison, x='Model', y='Processing Time (ms)',
            color='Processing Time (ms)',
            color_continuous_scale='reds_r'
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### âš¡ Efficiency Metrics")
        st.dataframe(df_comparison[['Model', 'Energy Efficiency', 'Memory Usage (MB)']], use_container_width=True)
        
        # Energy efficiency comparison
        fig = px.bar(
            df_comparison, x='Model', y='Energy Efficiency',
            color='Energy Efficiency',
            color_continuous_scale='greens'
        )
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)
    
    # Side-by-side results
    st.markdown("### ğŸ” Side-by-Side Visual Comparison")
    
    if st.session_state.uploaded_images:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.markdown("#### Original")
            st.image(st.session_state.uploaded_images[0]['data'], use_container_width=True)
        
        with col2:
            st.markdown("#### CNN Result")
            cnn_result = create_segmentation_overlay(st.session_state.uploaded_images[0]['data'], 'cnn')
            st.image(cnn_result, use_container_width=True)
        
        with col3:
            st.markdown("#### SNN Result")
            snn_result = create_segmentation_overlay(st.session_state.uploaded_images[0]['data'], 'snn')
            st.image(snn_result, use_container_width=True)
        
        with col4:
            st.markdown("#### Transformer Result")
            transformer_result = create_segmentation_overlay(st.session_state.uploaded_images[0]['data'], 'transformer')
            st.image(transformer_result, use_container_width=True)

def show_interactive_map():
    """Interactive mapping interface"""
    st.markdown('<h2 class="section-header">ğŸ—ºï¸ Interactive Remote Sensing Map</h2>', unsafe_allow_html=True)
    
    # Map configuration
    col1, col2 = st.columns([3, 1])
    
    with col2:
        st.markdown("### ğŸ›ï¸ Map Controls")
        
        # Location selection
        location = st.selectbox(
            "Select Region",
            ["Custom", "Amazon Rainforest", "Sahara Desert", "Great Lakes", "Himalayas", "California Coast"]
        )
        
        if location == "Custom":
            lat = st.number_input("Latitude", -90.0, 90.0, 40.7128)
            lon = st.number_input("Longitude", -180.0, 180.0, -74.0060)
        elif location == "Amazon Rainforest":
            lat, lon = -3.4653, -62.2159
        elif location == "Sahara Desert":
            lat, lon = 23.4162, 25.6628
        elif location == "Great Lakes":
            lat, lon = 44.7972, -86.7792
        elif location == "Himalayas":
            lat, lon = 27.9881, 86.9250
        else:  # California Coast
            lat, lon = 36.7783, -119.4179
        
        # Map layers
        st.markdown("#### ğŸ—‚ï¸ Map Layers")
        show_satellite = st.checkbox("Satellite Imagery", value=True)
        show_predictions = st.checkbox("AI Predictions", value=True)
        show_confidence = st.checkbox("Confidence Heatmap")
        show_boundaries = st.checkbox("Administrative Boundaries")
        
        # Analysis tools
        st.markdown("#### ğŸ”§ Analysis Tools")
        draw_roi = st.checkbox("Draw ROI")
        measure_area = st.checkbox("Measure Area") 
        export_data = st.checkbox("Export Results")
        
        if st.button("ğŸ”„ Refresh Analysis", type="primary"):
            st.success("Analysis updated!")
    
    with col1:
        # Create interactive map
        m = folium.Map(location=[lat, lon], zoom_start=10)
        
        # Add satellite layer
        if show_satellite:
            folium.TileLayer(
                tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',
                attr='ESRI',
                name='Satellite',
                overlay=False,
                control=True
            ).add_to(m)
        
        # Add prediction overlays
        if show_predictions:
            # Mock prediction data
            prediction_data = [
                {"lat": lat + 0.01, "lon": lon + 0.01, "class": "Forest", "confidence": 0.92},
                {"lat": lat - 0.01, "lon": lon - 0.01, "class": "Urban", "confidence": 0.87},
                {"lat": lat + 0.005, "lon": lon - 0.005, "class": "Water", "confidence": 0.95},
                {"lat": lat - 0.005, "lon": lon + 0.005, "class": "Agriculture", "confidence": 0.89},
            ]
            
            colors = {"Forest": "green", "Urban": "red", "Water": "blue", "Agriculture": "orange"}
            
            for pred in prediction_data:
                folium.CircleMarker(
                    location=[pred["lat"], pred["lon"]],
                    radius=8,
                    popup=f"{pred['class']}: {pred['confidence']:.2f}",
                    color=colors[pred["class"]],
                    fillColor=colors[pred["class"]],
                    fillOpacity=0.7
                ).add_to(m)
        
        # Add confidence heatmap
        if show_confidence:
            from folium.plugins import HeatMap
            
            # Mock confidence data
            heat_data = [[lat + np.random.uniform(-0.02, 0.02), 
                         lon + np.random.uniform(-0.02, 0.02), 
                         np.random.uniform(0.5, 1.0)] for _ in range(50)]
            
            HeatMap(heat_data, radius=15, max_zoom=18).add_to(m)
        
        # Layer control
        folium.LayerControl().add_to(m)
        
        # Display map
        map_data = st_folium(m, width=700, height=500)
        
        # Display clicked location info
        if map_data['last_clicked']:
            clicked_lat = map_data['last_clicked']['lat']
            clicked_lng = map_data['last_clicked']['lng']
            
            st.info(f"ğŸ“ Clicked: {clicked_lat:.4f}, {clicked_lng:.4f}")
            
            # Mock analysis for clicked location
            col1_info, col2_info, col3_info = st.columns(3)
            
            with col1_info:
                st.metric("Land Cover", "Forest")
            with col2_info:
                st.metric("Confidence", "94.2%")
            with col3_info:
                st.metric("NDVI", "0.78")

def show_advanced_settings():
    """Advanced configuration and settings page"""
    st.markdown('<h2 class="section-header">âš™ï¸ Advanced Settings</h2>', unsafe_allow_html=True)
    
    # Settings tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ›ï¸ Model Config", "ğŸ“Š Data Processing", "â˜ï¸ Cloud & Export", "ğŸ”§ System"])
    
    with tab1:
        st.markdown("### ğŸ§  Neural Network Configuration")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ”¥ CNN Settings")
            with st.expander("Layer Configuration"):
                cnn_architecture = st.selectbox("Architecture", ["ResNet", "DenseNet", "EfficientNet", "Custom"])
                cnn_pretrained = st.checkbox("Use Pretrained Weights", value=True)
                cnn_freeze_layers = st.slider("Freeze Layers", 0, 10, 5)
                cnn_learning_rate = st.select_slider("Learning Rate", options=[1e-5, 1e-4, 1e-3, 1e-2], value=1e-3)
            
            st.markdown("#### âš¡ SNN Settings")
            with st.expander("Spike Configuration"):
                snn_model = st.selectbox("Neuron Model", ["LIF", "Izhikevich", "AdEx", "SLIF"])
                snn_threshold = st.slider("Spike Threshold", 0.1, 2.0, 0.5, 0.1)
                snn_decay = st.slider("Membrane Decay", 0.1, 0.95, 0.7, 0.05)
                snn_reset = st.selectbox("Reset Type", ["Zero", "Subtract", "None"])
                snn_surrogate = st.selectbox("Surrogate Gradient", ["ATan", "Sigmoid", "SuperSpike"])
        
        with col2:
            st.markdown("#### ğŸ¯ Transformer Settings")
            with st.expander("Attention Configuration"):
                vit_model_size = st.selectbox("Model Size", ["Tiny", "Small", "Base", "Large"])
                vit_patch_size = st.selectbox("Patch Size", [8, 16, 32], index=1)
                vit_embed_dim = st.number_input("Embedding Dimension", 256, 1024, 512, 64)
                vit_num_heads = st.slider("Attention Heads", 4, 16, 8)
                vit_dropout = st.slider("Dropout Rate", 0.0, 0.5, 0.1, 0.05)
            
            st.markdown("#### ğŸ”„ Training Settings")
            with st.expander("Optimization Parameters"):
                optimizer = st.selectbox("Optimizer", ["Adam", "AdamW", "SGD", "RMSprop"])
                batch_size = st.selectbox("Batch Size", [8, 16, 32, 64], index=1)
                max_epochs = st.slider("Max Epochs", 10, 200, 50)
                early_stopping = st.checkbox("Early Stopping", value=True)
                if early_stopping:
                    patience = st.slider("Patience", 5, 20, 10)
    
    with tab2:
        st.markdown("### ğŸ“Š Data Processing Pipeline")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ–¼ï¸ Image Processing")
            
            # Augmentation settings
            with st.expander("Data Augmentation"):
                enable_aug = st.checkbox("Enable Augmentation", value=True)
                if enable_aug:
                    rotation = st.slider("Rotation (degrees)", 0, 45, 15)
                    flip_horizontal = st.checkbox("Horizontal Flip", value=True)
                    flip_vertical = st.checkbox("Vertical Flip")
                    brightness = st.slider("Brightness Variation", 0.0, 0.5, 0.2)
                    contrast = st.slider("Contrast Variation", 0.0, 0.5, 0.2)
                    gaussian_noise = st.slider("Gaussian Noise", 0.0, 0.1, 0.02)
            
            # Preprocessing pipeline
            with st.expander("Preprocessing Pipeline"):
                resize_method = st.selectbox("Resize Method", ["Bilinear", "Bicubic", "Lanczos", "Nearest"])
                normalization = st.selectbox("Normalization", ["ImageNet", "Custom", "Per-Image", "None"])
                
                if normalization == "Custom":
                    norm_mean = st.text_input("Mean (R,G,B)", "0.485,0.456,0.406")
                    norm_std = st.text_input("Std (R,G,B)", "0.229,0.224,0.225")
        
        with col2:
            st.markdown("#### ğŸ“¡ Spectral Processing")
            
            with st.expander("Band Operations"):
                available_bands = ["Red", "Green", "Blue", "NIR", "SWIR1", "SWIR2", "Thermal"]
                selected_bands = st.multiselect("Input Bands", available_bands, default=["Red", "Green", "Blue", "NIR"])
                
                # Spectral indices
                compute_ndvi = st.checkbox("Compute NDVI", value=True)
                compute_ndwi = st.checkbox("Compute NDWI")
                compute_nbr = st.checkbox("Compute NBR")
                compute_evi = st.checkbox("Compute EVI")
            
            with st.expander("Quality Control"):
                cloud_masking = st.checkbox("Cloud Masking", value=True)
                shadow_masking = st.checkbox("Shadow Masking")
                quality_threshold = st.slider("Quality Threshold", 0.0, 1.0, 0.8)
                
        # Processing summary
        st.markdown("#### ğŸ”„ Processing Summary")
        processing_config = {
            "Input Bands": len(selected_bands),
            "Augmentation": "Enabled" if enable_aug else "Disabled",
            "Normalization": normalization,
            "Quality Control": "Enabled" if cloud_masking or shadow_masking else "Disabled"
        }
        
        config_df = pd.DataFrame(list(processing_config.items()), columns=["Setting", "Value"])
        st.dataframe(config_df, use_container_width=True)
    
    with tab3:
        st.markdown("### â˜ï¸ Cloud & Export Settings")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### â˜ï¸ Cloud Processing")
            
            enable_cloud = st.checkbox("Enable Cloud Processing")
            if enable_cloud:
                cloud_provider = st.selectbox("Provider", ["AWS", "Google Cloud", "Azure", "Custom"])
                instance_type = st.selectbox("Instance Type", ["CPU", "GPU (T4)", "GPU (V100)", "GPU (A100)"])
                max_concurrent = st.slider("Max Concurrent Jobs", 1, 10, 3)
                
                if st.button("ğŸ”— Connect to Cloud"):
                    st.success("âœ… Connected to cloud processing!")
            
            st.markdown("#### ğŸ“Š Data Sources")
            
            # API connections
            enable_sentinel = st.checkbox("Sentinel Hub API", value=True)
            enable_landsat = st.checkbox("Landsat Collection API")
            enable_modis = st.checkbox("MODIS API")
            enable_planet = st.checkbox("Planet Labs API")
            
            if enable_sentinel:
                sentinel_key = st.text_input("Sentinel Hub API Key", type="password")
        
        with col2:
            st.markdown("#### ğŸ’¾ Export Configuration")
            
            # Output formats
            export_formats = st.multiselect(
                "Output Formats",
                ["GeoTIFF", "PNG", "JPG", "Shapefile", "GeoJSON", "KML", "CSV"],
                default=["GeoTIFF", "PNG"]
            )
            
            # Coordinate system
            crs = st.selectbox("Coordinate System", ["EPSG:4326", "EPSG:3857", "EPSG:32633", "Custom"])
            if crs == "Custom":
                custom_crs = st.text_input("Custom CRS")
            
            # Output quality
            output_quality = st.slider("Output Quality", 50, 100, 90)
            compress_output = st.checkbox("Compress Output", value=True)
            
            # Batch processing
            st.markdown("##### Batch Export")
            batch_size = st.slider("Batch Size", 1, 50, 10)
            auto_export = st.checkbox("Auto-export Results")
            
            if st.button("ğŸ“¦ Export Current Results"):
                create_export_package()
    
    with tab4:
        st.markdown("### ğŸ”§ System Configuration")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ’» Performance Settings")
            
            # Processing settings
            num_workers = st.slider("CPU Workers", 1, 16, 4)
            memory_limit = st.slider("Memory Limit (GB)", 4, 32, 8)
            gpu_enabled = st.checkbox("Enable GPU", value=True)
            
            if gpu_enabled:
                gpu_memory_growth = st.checkbox("GPU Memory Growth", value=True)
                mixed_precision = st.checkbox("Mixed Precision Training")
            
            # Caching
            st.markdown("##### ğŸ—„ï¸ Caching")
            enable_cache = st.checkbox("Enable Model Caching", value=True)
            cache_size = st.slider("Cache Size (GB)", 1, 10, 2)
            clear_cache = st.button("ğŸ—‘ï¸ Clear Cache")
            
            if clear_cache:
                st.success("Cache cleared successfully!")
        
        with col2:
            st.markdown("#### ğŸ“ˆ Monitoring & Logging")
            
            # Logging level
            log_level = st.selectbox("Log Level", ["DEBUG", "INFO", "WARNING", "ERROR"])
            log_to_file = st.checkbox("Log to File", value=True)
            
            # Metrics tracking
            track_metrics = st.checkbox("Track Performance Metrics", value=True)
            if track_metrics:
                metrics_interval = st.slider("Metrics Interval (seconds)", 1, 60, 10)
            
            # System info
            st.markdown("##### ğŸ’¾ System Information")
            
            system_info = {
                "Python Version": "3.9.7",
                "Streamlit Version": "1.28.0",
                "PyTorch Version": "2.0.1",
                "CUDA Available": "Yes" if gpu_enabled else "No",
                "Memory Usage": "4.2 GB / 16 GB",
                "GPU Memory": "2.1 GB / 8 GB" if gpu_enabled else "N/A"
            }
            
            for key, value in system_info.items():
                st.text(f"{key}: {value}")

def show_research_mode():
    """Research and benchmarking mode"""
    st.markdown('<h2 class="section-header">ğŸ“š Research Mode</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    ### ğŸ”¬ Advanced Research & Benchmarking Platform
    
    This mode provides comprehensive tools for researchers to evaluate and compare brain-inspired 
    models against traditional approaches in remote sensing applications.
    """)
    
    # Research tabs
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ§ª Experiments", "ğŸ“Š Benchmarks", "ğŸ“‹ Reports", "ğŸ“ Education"])
    
    with tab1:
        st.markdown("### ğŸ§ª Experiment Configuration")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ“ Experiment Setup")
            
            experiment_name = st.text_input("Experiment Name", "Brain_RS_Comparison_2024")
            experiment_desc = st.text_area("Description", "Comparing brain-inspired models for land cover classification")
            
            # Dataset selection
            st.markdown("##### ğŸ“Š Dataset")
            dataset = st.selectbox(
                "Select Dataset",
                ["EuroSAT", "UC Merced", "AID", "RESISC45", "BigEarthNet", "Custom Dataset"]
            )
            
            if dataset == "Custom Dataset":
                st.file_uploader("Upload Dataset", type=['zip', 'tar'])
            
            # Experiment parameters
            st.markdown("##### âš™ï¸ Parameters")
            cross_validation = st.checkbox("K-Fold Cross Validation", value=True)
            if cross_validation:
                k_folds = st.slider("K Folds", 3, 10, 5)
            
            test_split = st.slider("Test Split", 0.1, 0.3, 0.2)
            random_seed = st.number_input("Random Seed", 0, 9999, 42)
        
        with col2:
            st.markdown("#### ğŸ¯ Models to Compare")
            
            # Model selection
            models_to_test = st.multiselect(
                "Select Models",
                [
                    "ResNet50", "ResNet101", "DenseNet121", 
                    "EfficientNet-B0", "SNN-LIF", "SNN-SLIF", 
                    "Vision Transformer", "Swin Transformer",
                    "Hybrid CNN-SNN", "Capsule Network"
                ],
                default=["ResNet50", "SNN-LIF", "Vision Transformer"]
            )
            
            # Hyperparameter ranges
            st.markdown("##### ğŸ”§ Hyperparameter Search")
            hp_search = st.checkbox("Enable Hyperparameter Search")
            
            if hp_search:
                search_method = st.selectbox("Search Method", ["Grid Search", "Random Search", "Bayesian Optimization"])
                search_trials = st.slider("Max Trials", 10, 100, 25)
            
            # Metrics to evaluate
            st.markdown("##### ğŸ“Š Evaluation Metrics")
            metrics = st.multiselect(
                "Metrics",
                ["Accuracy", "Precision", "Recall", "F1-Score", "IoU", "AUC", "Cohen's Kappa", "Energy Consumption"],
                default=["Accuracy", "F1-Score", "IoU", "Energy Consumption"]
            )
            
            if st.button("ğŸš€ Start Experiment", type="primary"):
                run_research_experiment(experiment_name, models_to_test, metrics)
    
    with tab2:
        st.markdown("### ğŸ“Š Benchmark Results")
        
        # Benchmark comparison
        st.markdown("#### ğŸ† Model Performance Leaderboard")
        
        # Mock benchmark data
        benchmark_data = {
            'Model': ['Vision Transformer', 'SNN-SLIF', 'Hybrid CNN-SNN', 'ResNet50', 'EfficientNet-B0', 'SNN-LIF'],
            'Accuracy': [94.8, 93.2, 92.7, 91.5, 90.8, 89.6],
            'F1-Score': [0.947, 0.929, 0.925, 0.912, 0.905, 0.893],
            'IoU': [0.892, 0.876, 0.871, 0.858, 0.851, 0.842],
            'Energy (mJ)': [145, 23, 67, 156, 134, 34],
            'Inference Time (ms)': [89, 120, 78, 45, 52, 134],
            'Params (M)': [86.4, 12.3, 25.7, 23.5, 5.3, 8.9]
        }
        
        df_benchmark = pd.DataFrame(benchmark_data)
        
        # Interactive benchmark table
        st.dataframe(df_benchmark.style.highlight_max(axis=0, subset=['Accuracy', 'F1-Score', 'IoU']), use_container_width=True)
        
        # Performance visualization
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("##### ğŸ¯ Accuracy vs Energy Efficiency")
            fig = px.scatter(
                df_benchmark, x='Energy (mJ)', y='Accuracy',
                size='Params (M)', color='Model',
                hover_data=['F1-Score', 'IoU'],
                title="Accuracy vs Energy Consumption"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown("##### âš¡ Speed vs Accuracy")
            fig = px.scatter(
                df_benchmark, x='Inference Time (ms)', y='Accuracy',
                size='Params (M)', color='Model',
                title="Processing Speed vs Accuracy"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Detailed analysis
        st.markdown("#### ğŸ” Detailed Analysis")
        
        selected_model = st.selectbox("Select Model for Analysis", df_benchmark['Model'].tolist())
        model_data = df_benchmark[df_benchmark['Model'] == selected_model].iloc[0]
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Accuracy", f"{model_data['Accuracy']:.1f}%")
        with col2:
            st.metric("F1-Score", f"{model_data['F1-Score']:.3f}")
        with col3:
            st.metric("Energy", f"{model_data['Energy (mJ)']:.0f} mJ")
        with col4:
            st.metric("Speed", f"{model_data['Inference Time (ms)']:.0f} ms")
    
    with tab3:
        st.markdown("### ğŸ“‹ Research Reports")
        
        # Report generation
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("#### ğŸ“Š Auto-Generated Research Report")
            
            # Report configuration
            report_type = st.selectbox(
                "Report Type",
                ["Comprehensive Analysis", "Performance Comparison", "Energy Efficiency Study", "Custom Report"]
            )
            
            include_sections = st.multiselect(
                "Include Sections",
                ["Executive Summary", "Methodology", "Results", "Discussion", "Conclusion", "References"],
                default=["Executive Summary", "Results", "Discussion"]
            )
            
            # Report preview
            st.markdown("##### ğŸ“ Report Preview")
            
            with st.expander("Executive Summary"):
                st.markdown("""
                **Brain-Inspired Remote Sensing Analysis Report**
                
                This study compares brain-inspired neural architectures with traditional CNNs for remote sensing 
                image classification. Our experiments on the EuroSAT dataset demonstrate that Spiking Neural 
                Networks achieve competitive accuracy (93.2%) while consuming 85% less energy than traditional 
                approaches. Vision Transformers achieve the highest accuracy (94.8%) but require significantly 
                more computational resources.
                
                **Key Findings:**
                - SNNs provide excellent energy efficiency with minimal accuracy loss
                - Attention mechanisms significantly improve global context understanding
                - Hybrid architectures offer balanced performance across all metrics
                """)
            
            with st.expander("Results"):
                st.dataframe(df_benchmark, use_container_width=True)
                
                st.markdown("""
                **Statistical Analysis:**
                - Mean accuracy across all models: 92.1% (Â±1.8%)
                - Brain-inspired models show 67% better energy efficiency on average
                - Processing time varies from 45ms (ResNet) to 134ms (SNN-LIF)
                """)
        
        with col2:
            st.markdown("### ğŸ“¦ Export Options")
            
            # Export formats
            export_format = st.selectbox("Format", ["PDF", "LaTeX", "Word", "HTML", "Markdown"])
            include_figures = st.checkbox("Include Figures", value=True)
            include_data = st.checkbox("Include Raw Data", value=True)
            
            if st.button("ğŸ“„ Generate Report", type="primary"):
                generate_research_report(report_type, include_sections, export_format)
            
            st.markdown("### ğŸ“Š Citation")
            st.code("""
@article{brain_rs_2024,
  title={Brain-Inspired Remote Sensing: A Comparative Study},
  author={Research Team},
  journal={Remote Sensing Letters},
  year={2024},
  doi={10.1080/example}
}
            """)
    
    with tab4:
        st.markdown("### ğŸ“ Educational Mode")
        
        st.markdown("""
        #### ğŸ§  Understanding Brain-Inspired AI in Remote Sensing
        
        This educational section helps users understand the concepts behind brain-inspired 
        neural networks and their applications in remote sensing.
        """)
        
        # Educational content
        lesson = st.selectbox(
            "Select Lesson",
            [
                "Introduction to Spiking Neural Networks",
                "Attention Mechanisms in Vision",
                "Energy-Efficient Computing",
                "Remote Sensing Fundamentals",
                "Comparing AI Architectures"
            ]
        )
        
        if lesson == "Introduction to Spiking Neural Networks":
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                ##### ğŸ§  What are Spiking Neural Networks?
                
                Spiking Neural Networks (SNNs) are the third generation of neural networks that 
                more closely mimic biological neural networks. Unlike traditional artificial 
                neurons that output continuous values, spiking neurons communicate through 
                discrete spikes or pulses.
                
                **Key Characteristics:**
                - **Temporal Processing**: Information is encoded in the timing of spikes
                - **Energy Efficient**: Only active during spike events
                - **Biologically Plausible**: Mimics real neural behavior
                - **Event-Driven**: Asynchronous computation
                """)
            
            with col2:
                # Simple SNN visualization
                fig = go.Figure()
                
                # Simulate spike train
                time = np.linspace(0, 100, 1000)
                membrane_potential = np.zeros_like(time)
                spikes = []
                
                v = 0
                for i, t in enumerate(time):
                    v += np.random.normal(0.01, 0.005)  # Input current
                    v *= 0.99  # Decay
                    if v > 0.5:  # Threshold
                        spikes.append(t)
                        v = 0  # Reset
                    membrane_potential[i] = v
                
                fig.add_trace(go.Scatter(x=time, y=membrane_potential, mode='lines', name='Membrane Potential'))
                
                for spike_time in spikes:
                    fig.add_vline(x=spike_time, line_color='red', line_width=2, opacity=0.7)
                
                fig.update_layout(
                    title="Spiking Neuron Behavior",
                    xaxis_title="Time (ms)",
                    yaxis_title="Membrane Potential",
                    height=300
                )
                
                st.plotly_chart(fig, use_container_width=True)
        
        # Interactive quiz
        st.markdown("#### ğŸ§© Knowledge Check")
        
        quiz_question = st.radio(
            "What is the main advantage of Spiking Neural Networks in remote sensing applications?",
            [
                "Higher accuracy than traditional CNNs",
                "Energy-efficient processing suitable for edge devices",
                "Faster inference time",
                "Simpler architecture"
            ]
        )
        
        if st.button("Check Answer"):
            if quiz_question == "Energy-efficient processing suitable for edge devices":
                st.success("âœ… Correct! SNNs are particularly valuable for their energy efficiency.")
            else:
                st.error("âŒ Try again! Think about the biological inspiration behind SNNs.")

# Helper functions
def run_model_comparison(models):
    """Run comparison between selected models"""
    st.info(f"ğŸ”„ Running comparison for: {', '.join(models)}")
    progress = st.progress(0)
    
    results = {}
    for i, model in enumerate(models):
        progress.progress((i + 1) / len(models))
        # Simulate model execution
        results[model] = {
            'accuracy': np.random.uniform(0.85, 0.95),
            'processing_time': np.random.uniform(50, 150),
            'energy_consumption': np.random.uniform(20, 160)
        }
    
    st.success("âœ… Comparison complete!")
    return results

def run_research_experiment(name, models, metrics):
    """Run a comprehensive research experiment"""
    st.info(f"ğŸš€ Starting experiment: {name}")
    
    # Progress tracking
    progress = st.progress(0)
    status = st.empty()
    
    total_steps = len(models) * 5  # 5 steps per model
    current_step = 0
    
    for model in models:
        status.text(f"Training {model}...")
        for step in range(5):
            current_step += 1
            progress.progress(current_step / total_steps)
            # Simulate training steps
            time.sleep(0.1)
    
    status.text("âœ… Experiment completed!")
    st.success(f"Experiment '{name}' finished successfully!")

def generate_research_report(report_type, sections, format_type):
    """Generate a research report"""
    st.info(f"ğŸ“„ Generating {report_type} report in {format_type} format...")
    
    # Simulate report generation
    progress = st.progress(0)
    for i in range(100):
        progress.progress(i + 1)
        time.sleep(0.01)
    
    st.success("âœ… Report generated successfully!")
    
    # Create download button
    if st.button("ğŸ“¥ Download Report"):
        st.balloons()
        st.success("Report downloaded!")

def create_export_package():
    """Create export package with results"""
    st.info("ğŸ“¦ Creating export package...")
    
    # Simulate package creation
    progress = st.progress(0)
    files = ["predictions.geotiff", "results.csv", "visualization.png", "metadata.json"]
    
    for i, file in enumerate(files):
        progress.progress((i + 1) / len(files))
        st.text(f"Adding {file}...")
        time.sleep(0.2)
    
    st.success("âœ… Export package created!")
    
    # Mock download
    if st.button("ğŸ“¥ Download Package"):
        st.balloons()
        st.success("Package downloaded!")

# Mock preprocessing and visualization functions
def preprocess_image(image, bands=None, normalization=None, denoise=False, enhance_contrast=False):
    """Mock preprocessing function"""
    # Simple preprocessing simulation
    processed = image.copy()
    if enhance_contrast:
        processed = processed * 1.2
    if denoise:
        processed = np.clip(processed, 0, 1)
    return processed

def create_segmentation_overlay(image, model_type='default'):
    """Create mock segmentation overlay"""
    # Create random segmentation mask
    mask = np.random.randint(0, 5, (image.shape[0], image.shape[1]))
    overlay = image.copy()
    
    # Apply color coding
    colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1)]
    for i in range(5):
        overlay[mask == i] = colors[i]
    
    return overlay

def create_attention_map(image, attention_type='default'):
    """Create mock attention map"""
    # Generate random attention pattern
    attention = np.random.rand(image.shape[0], image.shape[1])
    
    # Apply Gaussian blur for smoothness
    from scipy.ndimage import gaussian_filter
    attention = gaussian_filter(attention, sigma=5)
    
    # Normalize and create heatmap
    attention = (attention - attention.min()) / (attention.max() - attention.min())
    
    # Create RGB heatmap
    heatmap = np.zeros((*attention.shape, 3))
    heatmap[:, :, 0] = attention  # Red channel
    heatmap[:, :, 1] = 1 - attention  # Green channel (inverse)
    
    return heatmap

if __name__ == "__main__":
    main()